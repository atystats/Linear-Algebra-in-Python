{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Decomposition (Matrix Factorization)\n",
    "A matrix decomposition is a way of reducing the matrix into its constituent parts. Two simple ways of decomposition are :-\n",
    "\n",
    "1. __LU decomposition__ :- This is for the square matrix and decomposes the matrix in 2 components L and U.\n",
    "$$A = L U$$\n",
    "where L is the lower traingle matrix and U is the upper traingle matrix.\n",
    "\n",
    "LU decomposition is found using an interative numerical process and can fail for those matrices that cannot be decomposed. Another variation of the decomposition is called LUP decomposition. \n",
    "$$A = L\\ U\\ P$$\n",
    "\n",
    "The rows of the parent matrix are re-ordered to simplify the decomposition process and the additional P matrix specifies a way to permute the result or return the result to the original order. There are also other variations of the LU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as lin\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "[[1.         0.         0.        ]\n",
      " [0.14285714 1.         0.        ]\n",
      " [0.57142857 0.5        1.        ]]\n",
      "[[ 7.00000000e+00  8.00000000e+00  9.00000000e+00]\n",
      " [ 0.00000000e+00  8.57142857e-01  1.71428571e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00 -1.58603289e-16]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,2,3],\n",
    "              [4,5,6],\n",
    "              [7,8,9]])\n",
    "\n",
    "# factorize \n",
    "P, L, U = lin.lu(A)\n",
    "print(P)\n",
    "print(L)\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "# We can reconstruct the matrix A\n",
    "print(P.dot(L).dot(U))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __QR Decomposition__ :- \n",
    "The QR decomposition is for n X M matrices and decomposes a matrix into Q and R components.\n",
    "$$A = Q\\ R$$\n",
    "where Q is a matrix of size m X m and R is an upper traingle matrix with size m X n. The use of QR is not limited to square matrix.\n",
    "\n",
    "QR decomposition is found using an interative numerical process and can fail for those matrices that cannot be decomposed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.16903085  0.89708523  0.40824829]\n",
      " [-0.50709255  0.27602622 -0.81649658]\n",
      " [-0.84515425 -0.34503278  0.40824829]]\n",
      "[[-5.91607978 -7.43735744]\n",
      " [ 0.          0.82807867]\n",
      " [ 0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,2],\n",
    "              [3,4],\n",
    "              [5,6]])\n",
    "\n",
    "# factorize \n",
    "Q, R = lin.qr(A, mode = \"full\")\n",
    "print(Q)\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# We can reconstruct the matrix A\n",
    "print(Q.dot(R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. __Cholesky Decomposition__ :- \n",
    "The Cholesky decomposition is for square symmetric matrices where all values are greater than zero, so-called positive definite matrices. The decomposition can be written as $$A = L\\ L^T$$\n",
    "where L is the lower triangular matrix and $L^T$ is the transpose of L. We can also write the decompose as $$A = U^T\\ U$$ where U is the upper triangular matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.41421356 0.         0.        ]\n",
      " [0.70710678 1.22474487 0.        ]\n",
      " [0.70710678 0.40824829 1.15470054]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[2,1,1],\n",
    "              [1,2,1],\n",
    "              [1,1,2]])\n",
    "L = lin.cholesky(A, lower = True)\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 1. 1.]\n",
      " [1. 2. 1.]\n",
      " [1. 1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "# We can reconstruct the matrix A\n",
    "print(L.dot(L.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigendecomposition\n",
    "This decomposes a square matrix into eigenvectors and eigenvalues. \n",
    "\n",
    "__Eigenvector__ :- A vector is an eigenvector of a matrix if it satisfies the following equation. \n",
    "$$A\\ v = \\lambda\\ v $$\n",
    "where v is the eigenvector and $\\lambda$ is the eigen value. Eigenvectors are unit vectors, which means that their length or magnitude is equal to 1.0.\n",
    "\n",
    "__Eigenvalues__ :- Eigenvalues are coefficients applied to eigenvectors that give the vectors their length or magnitude.\n",
    "\n",
    "A matrix could have one eigenvector and eigenvalue for each dimension of the parent matrix. Not all square matrices can be decomposed into eigenvectors and eigenvalues, and some can only be decomposed in a way that requires complex numbers.\n",
    "\n",
    "The parent matrix can be shown as a product of the eigenvectors and eigenvalues. $$A = Q\\ \\Lambda\\ Q^T$$\n",
    "where Q is a matrix comprised of eigenvectors and $\\Lambda$ is the diagonal matrix comprised of eigenvalues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.61168440e+01 -1.11684397e+00 -9.75918483e-16]\n",
      "[[-0.23197069 -0.78583024  0.40824829]\n",
      " [-0.52532209 -0.08675134 -0.81649658]\n",
      " [-0.8186735   0.61232756  0.40824829]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,2,3],\n",
    "              [4,5,6],\n",
    "              [7,8,9]])\n",
    "\n",
    "# Eigendecomposition\n",
    "values, vectors = np.linalg.eig(A)\n",
    "print(values)\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm if a vector is an eigenvector or not by using $A\\ v = \\lambda\\ v $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -3.73863537,  -8.46653421, -13.19443305])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.dot(vectors[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -3.73863537,  -8.46653421, -13.19443305])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[0] * vectors[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "# We can reconstruct the matrix as \n",
    "Q = vectors # matrix comprised of eigen vectors\n",
    "L = np.diag(values) # create a diagonal matrix with eigenvalues.\n",
    "R = np.linalg.inv(Q)\n",
    "\n",
    "B = Q.dot(L).dot(R)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition\n",
    "SVD decomposes a matrix such that\n",
    "$$A = U\\ \\Sigma\\ V^T$$\n",
    "\n",
    "A is the n X m matrix that we wish to decompose. U is and m X m matrix (columns of U are called left-singular vectors of A), $\\Sigma$ is an m X n matrix ( the diagonal of this matrix are called singular values of A) and V is a n X n matrix (columns of V are called right-singular vectors of A). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.2298477   0.88346102  0.40824829]\n",
      " [-0.52474482  0.24078249 -0.81649658]\n",
      " [-0.81964194 -0.40189603  0.40824829]]\n",
      "[9.52551809 0.51430058]\n",
      "[[-0.61962948 -0.78489445]\n",
      " [-0.78489445  0.61962948]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "  [1, 2],\n",
    "  [3, 4],\n",
    "  [5, 6]])\n",
    "\n",
    "# Decomposition of A\n",
    "U,s,V = lin.svd(A)\n",
    "print(U)\n",
    "print(s)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct the matrix\n",
    "Sigma = np.zeros((A.shape[0], A.shape[1])) # this step is only require for non-square matrix\n",
    "Sigma [:A.shape[1], :A.shape[1]] = np.diag(s) # assigning singular values to diagonal\n",
    "\n",
    "B = U.dot(Sigma.dot(V))\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudoinverse\n",
    "Also known as Generalized Inverse. It can be calculated even for the non-square metrix as well. It is denoted as $A^+$. \n",
    "$$A^+ = V\\ D^+\\ U^T$$\n",
    "where $D^+$ = Generalised inverse of $\\Sigma$ (diagonal matrix). U and V are same as in SVD. \n",
    "If $$\\Sigma = \\begin{pmatrix} S_{1,1} & 0 & 0 \\\\\n",
    "0 & S_{2,2} & 0\\\\\n",
    "0 & 0 & S_{3,3}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "$$D^+ = \\begin{pmatrix} \\frac{1}{S_{1,1}} & 0 & 0 \\\\\n",
    "0 & \\frac{1}{S_{2,2}} & 0\\\\\n",
    "0 & 0 & \\frac{1}{S_{3,3}}\n",
    "\\end{pmatrix}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.00000000e+01 -5.00000000e+00  9.07607323e-15  5.00000000e+00]\n",
      " [ 8.50000000e+00  4.50000000e+00  5.00000000e-01 -3.50000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "  [0.1, 0.2],\n",
    "  [0.3, 0.4],\n",
    "  [0.5, 0.6],\n",
    "  [0.7, 0.8]])\n",
    "\n",
    "# Calculating pseudoinverse\n",
    "B = np.linalg.pinv(A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.00000000e+01 -5.00000000e+00  9.07607323e-15  5.00000000e+00]\n",
      " [ 8.50000000e+00  4.50000000e+00  5.00000000e-01 -3.50000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#Calculating pseudoinverse using SVD.\n",
    "U,s,V = lin.svd(A)\n",
    "d = 1/s\n",
    "\n",
    "# Calculating the D matrix with inverse of singular value\n",
    "D = np.zeros(A.shape)\n",
    "D[:A.shape[1], :A.shape[1]] = np.diag(d)\n",
    "\n",
    "B = V.T.dot(D.T).dot(U.T)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "We can use SVD to reduce the data with a large number of features to a smaller subset of features that are most relevant to the prediction problem.\n",
    "\n",
    "To do this we use SVD on the original data and select the top largest singular values in $\\Sigma$. These columns\n",
    "can be selected from $\\Sigma$ and rows can be selected from $V^T$.\n",
    "\n",
    "An approximation B of the original vector can be written as $$B = U\\ \\Sigma_k\\ V_k^T$$\n",
    "\n",
    "In practice, we can retain and work with a descriptive subset of the data called T. This is a dense summary of the matrix or a projection.\n",
    "\n",
    "$$T = U\\ \\Sigma_k$$\n",
    "Further, this transform can be calculated and applied to the original matrix A as well as other similar matrices.\n",
    "$$T = A\\ V_k^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      " [11. 12. 13. 14. 15. 16. 17. 18. 19. 20.]\n",
      " [21. 22. 23. 24. 25. 26. 27. 28. 29. 30.]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([list(range(1,11)), list(range(11,21)), list(range(21,31))])\n",
    "U,s,V = lin.svd(A)\n",
    "\n",
    "Sigma = np.zeros((A.shape[0], A.shape[1]))\n",
    "Sigma [:A.shape[0], :A.shape[0]] = np.diag(s)\n",
    "\n",
    "#dimension to be reduced to\n",
    "k = 2\n",
    "Sigma = Sigma[:, :k]\n",
    "V = V[:k,:]\n",
    "\n",
    "#Reconstruct the original matrix\n",
    "B = U.dot(Sigma.dot(V))\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-18.52157747   6.47697214]\n",
      " [-49.81310011   1.91182038]\n",
      " [-81.10462276  -2.65333138]]\n",
      "[[-18.52157747   6.47697214]\n",
      " [-49.81310011   1.91182038]\n",
      " [-81.10462276  -2.65333138]]\n"
     ]
    }
   ],
   "source": [
    "# dense summary of the matrix or a projection.\n",
    "B = U.dot(Sigma)\n",
    "print(B)\n",
    "\n",
    "T = A.dot(V.T)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18.52157747  6.47697214]\n",
      " [49.81310011  1.91182038]\n",
      " [81.10462276 -2.65333138]]\n"
     ]
    }
   ],
   "source": [
    "# Dimension reduction using sklearn\n",
    "svd = TruncatedSVD(n_components = 2)\n",
    "svd.fit(A)\n",
    "\n",
    "print(svd.transform(A))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
